---
title: 'Explain your ML model: no more black boxes üéÅ'
permalink: /posts/explain-ml/
tags:
  - explainability
  - interpretable ML
---
<h2>1. What's in a black box?</h2>
<div style="text-align: justify;">The more companies are interested in using machine learning and big data in their work, the more they care about the interpretability of the models. This is understandable: asking questions and looking for explanations is human.</div>
<div style="text-align: justify;">We want to know not only "What's the prediction?", but "Why so?" as well. Thus, interpretation of ML models is important and helps us to:</div>

<ul>
    <li>Explain individual predictions</li>
    <li>Understand models' behaviour</li>
    <li>Detect errors & biases</li>
    <li>Generate insights about data & create new features</li>
</ul>
<br>
<img src="/images/ml-workflow.png">
<br>
<h2>2. Different types of interpretation</h2>
<div style="text-align: justify;">Model's predictions can be explained in different ways. The choice of media relies on what would be the most appropriate for a given problem.</div>
<h3>Visualization</h3>
<div style="text-align: justify;">For example, visualized interpretations are perfect for explaining the image classifier predictions.</div>
<br>
<img src="/images/dog-viz.png">
<div style="text-align: justify;">Source: <a href="https://marcotcr.github.io/lime/tutorials/Tutorial%20-%20images.html"> LIME Tutorial </a></div>
<br>

<h3>Textual description</h3>
<div style="text-align: justify;">A brief text explanations is also an option.</div>
<br>
<img src="/images/text-desc.png">
<br>

<h3>Formulae</h3>
<div style="text-align: justify;">And sometimes an old, good formula is worth a thousand of words:</div>
$House price = 2800 * room + 10000 * {swimming pool} + $5000 * garage$
<br>


<h2>3. Trade-off between Accuracy and Interpretability</h2>
<div style="text-align: justify;">The thing is that not all kinds of machine learning models are equally interpretable. As a rule, more accurate and advanced algorithms, e.g. neural networks, are hard to explain. Imagine making sense of all these layers' weights!</div>

<div style="text-align: justify;">Thus, it is a job of a data scientist to:</div>
<ol>
  <li>Find a trade-off between accuracy and interpretability.</li>

  <div style="text-align: justify;">One may use a linear regression which predictions are easy to explain. But the price for a high interpretability may be a lower metric as compared to a more complicated boosting.</div>
  <li>Explain a choice of a particular algorithm to a client.</li>

</ol>

<br>
<img src="/images/tradeoff.png">
<br>

<h2>4. Feature importance</h2>
<div style="text-align: justify;">Feature importance helps to answer the question "What features affect the model's prediction?"</div>
<div style="text-align: justify;">One of the methods used to estimate the importance of features is Permutation importance.</div>
<div style="text-align: justify;">Idea: if we permute the values of an important feature, the data won't reflect the real world anymore and the accuracy of the model will go down.</div>
<div style="text-align: justify;">The method work as follows:</div>

<ul>
    <li>Train the model</li>
    <li>Mix up all values of the feature X. Make a prediction on an updated data.</li>
    <li>Compute  $Importance(X) = Accuracy_{actual} ‚àí Accuracy_{permutated}$.</li>
    <li>Restore the actual order of the feature's values. Repeat steps 2-3 with a next feature.</li>
</ul>

<b>Advantages</b>:

<ul>
    <li>Concise global explanation of the model's behaviour.</li>
    <li>Easy to interpret.</li>
    <li>No need to re-train a model again and again.</li>
</ul>

<b>Disadvantages</b>:
<ul>
  <li>Need the ground truth values for the target.</li>
  <li>Connection to a model's error. It's not always bad, simply not something we need in some cases.</li>
</ul>

<div style="text-align: justify;">Sometimes we want to know how much the prediction will change depending on the feature's value without taking into account how much the metric will change.</div>

<h2>5. Dependency plots</h2>
<div style="text-align: justify;">Partial Dependency Plots will help you to answer the question "How does the feature affect the predictions?" PDP provides a quick look on the global relationship between the feature and the prediction. The plot below shows that the more money is at one's bank account, the higher the probability of one signing a term deposit during <a href="http://https//archive.ics.uci.edu/ml/datasets/Bank+Marketing">a bank campaign</a>.</div>
<br>
<img src="/images/pdp.png">
<br>

<div style="text-align: justify;">Let's look at how this plot is created:</div>
<ol>
  <li>Take one sample: a single student, no loans, balance is around $1000.</li>
  <li>Increase the latter feature up to 5000.</li>
  <li>Make a prediction on an updated sample.</li>
  <li>What is the model output if balance==10? And so on.</li>
  <li>Moving along the x axis, from smaller to larger values, plot the resulting predictions on the y axis.</li>
</ol>


<div style="text-align: justify;">Now, we considered only one sample. To create a PDP, we need to repeat this procedure for all the samples, i.e. all the rows in our dataset, and then draw the average prediction.</div>

<b>Advantages</b>:

<ul>
    <li>Easy to interpret.</li>
    <li>Enables the interpretation of causality</li>
</ul>

<b>Disadvantages</b>:
<ul>
  <li>One plot can give you the analysis of only one or two features. Plots with more features would be difficult for humans to comprehend.</li>
  <li>An assumption of the independent features. However, this assumption is often violated in real life.</li>
  Why is this a problem? Imagine that we want to draw a PDP for the data with correlated features. While we change the values of one feature, the values of the related feature stay the same. As a result, we can get unrealistic data points. For instance, we are interested in the feature Weight, but the dataset also contains such a feature as Height. As we change the value of Weight, the value of Height is fixed so we can end up having a sample with Weight==200 kg and Height==150 cm.
  <li>Opposite effects can cancel out the feature's impact.</li>
  Imagine that a half of the values of a particular feature is positively correlated with the target: the higher the value, the higher the model's outcome. On the other hand, a half of the values is negatively correlated with the target: the lower the feature's value, the higher the prediction. In this case, a PDP may be a horizontal line since the positive effects got cancelled out by the negative ones.
</ul>

<h2>6. Local interpretation</h2>
<div style="text-align: justify;">For now, we have considered two methods of global interpretation: feature importance and dependecy plots. These approaches help us to explain our model's behaviour, well, at a global level which is surely nice. However, we often need to explain a particular prediction for an individual sample. To achieve this goal, we may turn to local interpretation. One technique that can be used here is LIME, Local Interpretable Model-agnostic Explanations.</div>

<div style="text-align: justify;">The idea is as follows: instead of interpreting predictions of the black box we have at hand, we create a local surrogate model which is interpretable by its nature (e.g. a linear regression or a decision tree), use it to predict on an interesting data point and finally explain the prediction.</div>

<br>
<img src="/images/lime.png">
<br>

<div style="text-align: justify;">On the picture above, the prediction to explain is a big red cross. Blue and pink areas represent the complex decision function of the black box model. Surely, this cannot be approximated by a linear model. However, as we see, the dashed line that represents the learned explanation is locally faithful.</div>
Source: <a href="https://arxiv.org/pdf/1602.04938.pdf">Why Should I Trust You?</d

<b>Advantages</b>:

<ul>
    <li>Concise and clear explanations.</li>
    <li>Compatible with most of data types: texts, images, tabular data.</li>
    <li>The speed of computation as we focus on one sample at a time.</li>
</ul>

<b>Disadvantages</b>:
<ul>
  <li>Only linear models are used to approximate the model's local behaviour.</li>
  <li>No global explanations.</li>
</ul>


<h2>7. SHAP</h2>
<div style="text-align: justify;"><a href="https://github.com/slundberg/shap">SHapley Additive exPlantions (SHAP)</a> is an method based on the concept of the Shapley values from the game theory.</div>

<div style="text-align: justify;">Idea: a feature is a "player", a prediction is a "gain". Then the Shapley value is the contribution of a feature averaged over all possible combinations of a "team":</div>

$$\phi_i(v) = \sum_{ S \subseteq N \setminus \lbrace i \rbrace } {{|S| ! ( N - |S| - 1 )!} \over {N!}} ( v( S \cup \lbrace i \rbrace) - v( S ))$$


$N$ - all players.

$S$ - the "team" of $N$ players.

$v(S)$ - the gain of $S$.

$v( S \cup \lbrace i \rbrace) - v(S)$ - the "player's" contribution when joining $S$.

<b>Advantages</b>:

<ul>
    <li>Global and local interpretation.</li>
    <li>Intuitively clear local explanations: the prediction is represented as a game outcome where the features are the team players.</li>
</ul>

<b>Disadvantages</b>:
<ul>
  <li>Shap returns only one value for each feature, not an interpretable model as LIME does.</li>
  <li>Slow when creating a global interpretation.</li>
</ul>

P.S. A quick (but totally not comprehensive!) overview of some tools for interpretable ML.
<br>
<img src="/images/lime.png">
<br>
