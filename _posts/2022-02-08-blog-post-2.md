---
title: 'Explain your ML model: no more black boxes üéÅ'
permalink: /posts/2022/02/explain-ml/
tags:
  - explainability
  - interpretable ML
---
<h2>1. What's in a black box?</h2>
<div style="text-align: justify;">The more companies are interested in using machine learning and big data in their work, the more they care about the interpretability of the models. This is understandable: asking questions and looking for explanations is human.</div>
<div style="text-align: justify;">We want to know not only "What's the prediction?", but "Why so?" as well. Thus, interpretation of ML models is important and helps us to:</div>

<ul>
    <li>Explain individual predictions</li>
    <li>Understand models' behaviour</li>
    <li>Detect errors & biases</li>
    <li>Generate insights about data & create new features</li>
</ul>
<br>
<img src="/images/ml-workflow.png">
<br>
<h2>2. Different types of interpretation</h2>
<div style="text-align: justify;">Model's predictions can be explained in different ways. The choice of media relies on what would be the most appropriate for a given problem.</div>
<br>
<h3>Visualization</h3>
<div style="text-align: justify;">For example, visualized interpretations are perfect for explaining the image classifier predictions.</div>
<br>
<img src="/images/dog-viz.png">
Source: <a href="URL"> LIME Tutorial </a>
<br>

<h3>Textual description</h3>
<div style="text-align: justify;">A brief text explanations is also an option.</div>
<img src="/images/text-desc.png">
<br>

<h3>Formulae</h3>
<div style="text-align: justify;">And sometimes an old, good formula is worth a thousand of words:</div>
$House price = $$2800 * room + $$10000 * swimming_pool + $$5000 * garage$
<br>


<h2>3. Trade-off between Accuracy and Interpretability</h2>
<div style="text-align: justify;">The thing is that not all kinds of machine learning models are equally interpretable. As a rule, more accurate and advanced algorithms, e.g. neural networks, are hard to explain. Imagine making sense of all these layers' weights!</div>

<div style="text-align: justify;">Thus, it is a job of a data scientist to:</div>
<ol>
  <li>Find a trade-off between accuracy and interpretability.</li>

  <div style="text-align: justify;">One may use a linear regression which predictions are easy to explain. But the price for a high interpretability may be a lower metric as compared to a more complicated boosting.</div>
  <li><p></p></li>
  <li>Explain a choice of a particular algorithm to a client.</li>

</ol>

<br>
<img src="/images/tradeoff.png">
<br>

<h2>4. Feature importance</h2>
<div style="text-align: justify;">Feature importance helps to answer the question "What features affect the model's prediction?"</div>
<div style="text-align: justify;">One of the methods used to estimate the importance of features is Permutation importance.</div>
<div style="text-align: justify;">Idea: if we permute the values of an important feature, the data won't reflect the real world anymore and the accuracy of the model will go down.</div>
<div style="text-align: justify;">The method work as follows:</div>

<ul>
    <li>Train the model</li>
    <li>Mix up all values of the feature X. Make a prediction on an updated data.</li>
    <li>Compute  $Importance(X) = Accuracy_{actual} ‚àí Accuracy_{permutated}$.</li>
    <li>Restore the actual order of the feature's values. Repeat steps 2-3 with a next feature.</li>
</ul>

<b>Advantages</b>:

<ul>
    <li>Concise global explanation of the model's behaviour.</li>
    <li>Easy to interpret.</li>
    <li>No need to re-train a model again and again.</li>
</ul>

<b>Disadvantages</b>:
<ul>
  <li>Need the ground truth values for the target.</li>
  <li>Connection to a model's error. It's not always bad, simply not something we need in some cases.</li>
</ul>
<div style="text-align: justify;">Sometimes we want to know how much the prediction will change depending on the feature's value without taking into account how much the metric will change.</div>
