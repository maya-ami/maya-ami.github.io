---
title: 'Explain your ML model: no more black boxes üéÅ'
permalink: /posts/2022/02/explain-ml/
tags:
  - explainability
---
1. What's in a black box?
=========
<div style="text-align: justify;">The more companies are interested in using machine learning and big data in their work, the more they care about the interpretability of the models. This is understandable: asking questions and looking for explanations is human.</div>
<p>
<div style="text-align: justify;">We want to know not only "What's the prediction?", but "Why so?" as well. Thus, interpretation of ML models is important and helps us to:</div>
* Explain individual predictions
* Understand models' behaviour
* Detect errors & biases
* Generate insights about data & create new features

![image](/images/ml-workflow.png)

2. Different types of interpretation
=========
Model's predictions can be explained in different ways. The choice of media relies on what would be the most appropriate for a given problem.

Visualization
-------
For example, visualized interpretations are perfect for explaining the image classifier predictions.
![image](/images/dog-viz.png)
Source: <a href="URL"> LIME Tutorial </a>

Textual description
-------
A brief text explanations is also an option.
![image](/images/text-desc.png)

Formulae
-------
And sometimes an old, good formula is worth a thousand of words:

House price = $2800 room + $10000 {swimming pool} + $5000 * garage$

3. Trade-off between Accuracy and Interpretability
=========
<div style="text-align: justify;">The thing is that not all kinds of machine learning models are equally interpretable. As a rule, more accurate and advanced algorithms, e.g. neural networks, are hard to explain. Imagine making sense of all these layers' weights!</div>

<div style="text-align: justify;">Thus, it is a job of a data scientist to:

1. Find a trade-off between accuracy and interpretability.

<div style="text-align: justify;">One may use a linear regression which predictions are easy to explain. But the price for a high interpretability may be a lower metric as compared to a more complicated boosting.</div>

2. Explain a choice of a particular algorithm to a client.

![image](/images/tradeoff.png)

4. Feature importance
=========
<div style="text-align: justify;">Feature importance helps to answer the question "What features affect the model's prediction?"</div>

<div style="text-align: justify;">One of the methods used to estimate the importance of features is Permutation importance.</div>

<div style="text-align: justify;">Idea: if we permute the values of an important feature, the data won't reflect the real world anymore and the accuracy of the model will go down.</div>

The method work as follows:

* Train the model
* Mix up all values of the feature X. Make a prediction on an updated data.
* Compute  $Importance(X) = Accuracy_{actual} ‚àí Accuracy_{permutated}$.
* Restore the actual order of the feature's values. Repeat steps 2-3 with a next feature.

Advantages:

* Concise global explanation of the model's behaviour.
* Easy to interpret.
* No need to re-train a model again and again.

Disadvantages:

* Need the ground truth values for the target.
* Connection to a model's error. It's not always bad, simply not something we need in some cases.

<div style="text-align: justify;">Sometimes we want to know how much the prediction will change depending on the feature's value without taking into account how much the metric will change.</div>
