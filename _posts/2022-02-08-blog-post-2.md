---
title: 'Explain your ML model: no more black boxes üéÅ'
permalink: /posts/2022/02/explain-ml/
tags:
  - explainability
  - interpretable ML
---
1. What's in a black box?
======
<div style="text-align: justify;">The more companies are interested in using machine learning and big data in their work, the more they care about the interpretability of the models. This is understandable: asking questions and looking for explanations is human.</div>
<p>
<div style="text-align: justify;">We want to know not only "What's the prediction?", but "Why so?" as well. Thus, interpretation of ML models is important and helps us to:</div>

<ul>
    <li>Explain individual predictions</li>
    <li>Understand models' behaviour</li>
    <li>Detect errors & biases</li>
    <li>Generate insights about data & create new features</li>
</ul>

<img src="/images/ml-workflow.png">

2. Different types of interpretation
======
Model's predictions can be explained in different ways. The choice of media relies on what would be the most appropriate for a given problem.

Visualization
------
For example, visualized interpretations are perfect for explaining the image classifier predictions.

<img src="/images/dog-viz.png">

Source: <a href="URL"> LIME Tutorial </a>

Textual description
------
A brief text explanations is also an option.
<img src="/images/text-desc.png">


Formulae
------
And sometimes an old, good formula is worth a thousand of words:

House price = $2800 * room + $10000 * {swimming pool} + $5000 * garage$

3. Trade-off between Accuracy and Interpretability
======
<div style="text-align: justify;">The thing is that not all kinds of machine learning models are equally interpretable. As a rule, more accurate and advanced algorithms, e.g. neural networks, are hard to explain. Imagine making sense of all these layers' weights!</div>

<div style="text-align: justify;">Thus, it is a job of a data scientist to:

1. Find a trade-off between accuracy and interpretability.

<div style="text-align: justify;">One may use a linear regression which predictions are easy to explain. But the price for a high interpretability may be a lower metric as compared to a more complicated boosting.</div>

2. Explain a choice of a particular algorithm to a client.

<img src="/images/tradeoff.png">

4. Feature importance
======
<div style="text-align: justify;">Feature importance helps to answer the question "What features affect the model's prediction?"</div>

<div style="text-align: justify;">One of the methods used to estimate the importance of features is Permutation importance.</div>

<div style="text-align: justify;">Idea: if we permute the values of an important feature, the data won't reflect the real world anymore and the accuracy of the model will go down.</div>

The method work as follows:

<ul>
    <li>Train the model</li>
    <li>Mix up all values of the feature X. Make a prediction on an updated data.</li>
    <li>Compute  $Importance(X) = Accuracy_{actual} ‚àí Accuracy_{permutated}$.</li>
    <li>Restore the actual order of the feature's values. Repeat steps 2-3 with a next feature.</li>
</ul>

<b>Advantages</b>:

<ul>
    <li>Concise global explanation of the model's behaviour.</li>
    <li>Easy to interpret.</li>
    <li>No need to re-train a model again and again.</li>
</ul>

<b>Disadvantages</b>:
<ul>
  <li>Need the ground truth values for the target.</li>
  <li>Connection to a model's error. It's not always bad, simply not something we need in some cases.</li>
</ul>
<div style="text-align: justify;">Sometimes we want to know how much the prediction will change depending on the feature's value without taking into account how much the metric will change.</div>
